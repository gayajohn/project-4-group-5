{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a3cd8d",
   "metadata": {},
   "source": [
    "# Machine Learning Models - LRA,LDA,MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905a040",
   "metadata": {},
   "source": [
    "### Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d593edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3932b1f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>industry</th>\n",
       "      <th>wr1</th>\n",
       "      <th>wr2</th>\n",
       "      <th>wr3</th>\n",
       "      <th>wr4</th>\n",
       "      <th>vol1</th>\n",
       "      <th>vol2</th>\n",
       "      <th>vol3</th>\n",
       "      <th>vol4</th>\n",
       "      <th>pe_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>free_cash_margin</th>\n",
       "      <th>volatility</th>\n",
       "      <th>cpi</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>sma</th>\n",
       "      <th>rsi</th>\n",
       "      <th>ema</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>-0.019375</td>\n",
       "      <td>0.042774</td>\n",
       "      <td>-0.009555</td>\n",
       "      <td>-0.007692</td>\n",
       "      <td>6230100.0</td>\n",
       "      <td>6996900.0</td>\n",
       "      <td>6377700.0</td>\n",
       "      <td>7026700.0</td>\n",
       "      <td>30.01</td>\n",
       "      <td>...</td>\n",
       "      <td>14.40</td>\n",
       "      <td>13.74</td>\n",
       "      <td>0.025251</td>\n",
       "      <td>241.428</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.0</td>\n",
       "      <td>56.240000</td>\n",
       "      <td>8.149406</td>\n",
       "      <td>56.300000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Utilities</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>-0.006826</td>\n",
       "      <td>0.031470</td>\n",
       "      <td>175800.0</td>\n",
       "      <td>195300.0</td>\n",
       "      <td>233900.0</td>\n",
       "      <td>221400.0</td>\n",
       "      <td>17.58</td>\n",
       "      <td>...</td>\n",
       "      <td>11.15</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.013118</td>\n",
       "      <td>238.132</td>\n",
       "      <td>0.36</td>\n",
       "      <td>5.0</td>\n",
       "      <td>73.803333</td>\n",
       "      <td>87.796610</td>\n",
       "      <td>73.714583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Real Estate</td>\n",
       "      <td>0.057182</td>\n",
       "      <td>-0.004482</td>\n",
       "      <td>-0.040843</td>\n",
       "      <td>0.036994</td>\n",
       "      <td>1298900.0</td>\n",
       "      <td>3493600.0</td>\n",
       "      <td>571100.0</td>\n",
       "      <td>832500.0</td>\n",
       "      <td>62.13</td>\n",
       "      <td>...</td>\n",
       "      <td>12.97</td>\n",
       "      <td>29.98</td>\n",
       "      <td>0.019104</td>\n",
       "      <td>237.838</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.196667</td>\n",
       "      <td>88.260870</td>\n",
       "      <td>23.240833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Industrials</td>\n",
       "      <td>-0.044857</td>\n",
       "      <td>0.062964</td>\n",
       "      <td>0.055392</td>\n",
       "      <td>-0.058078</td>\n",
       "      <td>1071300.0</td>\n",
       "      <td>880100.0</td>\n",
       "      <td>1080400.0</td>\n",
       "      <td>994800.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>...</td>\n",
       "      <td>27.43</td>\n",
       "      <td>-18.14</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>305.691</td>\n",
       "      <td>5.08</td>\n",
       "      <td>3.5</td>\n",
       "      <td>235.736667</td>\n",
       "      <td>11.037986</td>\n",
       "      <td>235.240833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consumer Staples</td>\n",
       "      <td>0.022806</td>\n",
       "      <td>-0.021995</td>\n",
       "      <td>0.021842</td>\n",
       "      <td>-0.028000</td>\n",
       "      <td>6563300.0</td>\n",
       "      <td>6745100.0</td>\n",
       "      <td>9474700.0</td>\n",
       "      <td>10172500.0</td>\n",
       "      <td>10.28</td>\n",
       "      <td>...</td>\n",
       "      <td>4.55</td>\n",
       "      <td>10.21</td>\n",
       "      <td>0.019597</td>\n",
       "      <td>259.101</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.2</td>\n",
       "      <td>63.633333</td>\n",
       "      <td>72.448980</td>\n",
       "      <td>63.837083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 industry       wr1       wr2       wr3       wr4       vol1  \\\n",
       "0  Consumer Discretionary -0.019375  0.042774 -0.009555 -0.007692  6230100.0   \n",
       "1               Utilities  0.025644  0.011924 -0.006826  0.031470   175800.0   \n",
       "2             Real Estate  0.057182 -0.004482 -0.040843  0.036994  1298900.0   \n",
       "3             Industrials -0.044857  0.062964  0.055392 -0.058078  1071300.0   \n",
       "4        Consumer Staples  0.022806 -0.021995  0.021842 -0.028000  6563300.0   \n",
       "\n",
       "        vol2       vol3        vol4  pe_ratio  ...  profit_margin  \\\n",
       "0  6996900.0  6377700.0   7026700.0     30.01  ...          14.40   \n",
       "1   195300.0   233900.0    221400.0     17.58  ...          11.15   \n",
       "2  3493600.0   571100.0    832500.0     62.13  ...          12.97   \n",
       "3   880100.0  1080400.0    994800.0      6.68  ...          27.43   \n",
       "4  6745100.0  9474700.0  10172500.0     10.28  ...           4.55   \n",
       "\n",
       "   free_cash_margin  volatility      cpi  interest_rate  unemployment_rate  \\\n",
       "0             13.74    0.025251  241.428           0.40                5.0   \n",
       "1              1.49    0.013118  238.132           0.36                5.0   \n",
       "2             29.98    0.019104  237.838           0.13                5.0   \n",
       "3            -18.14    0.023650  305.691           5.08                3.5   \n",
       "4             10.21    0.019597  259.101           0.10               10.2   \n",
       "\n",
       "          sma        rsi         ema  label  \n",
       "0   56.240000   8.149406   56.300000      0  \n",
       "1   73.803333  87.796610   73.714583      0  \n",
       "2   23.196667  88.260870   23.240833      0  \n",
       "3  235.736667  11.037986  235.240833      0  \n",
       "4   63.633333  72.448980   63.837083      1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ML_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f075f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbca80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c747a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the y variable, the labels\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Separate the X variable, the features\n",
    "X = df.drop(columns=\"label\")\n",
    "\n",
    "# One-hot encode the 'industry' column\n",
    "X = pd.get_dummies(X, columns=['industry'])\n",
    "\n",
    "X.head()\n",
    "\n",
    "# Splitting into Train and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "# Creating StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting Standard Scaller\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b8ba0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.43861084,  0.67699166, -1.54774223, ..., -0.46708284,\n",
       "        -0.1649277 , -0.32487997],\n",
       "       [-0.04102101, -3.88491824, -0.29083965, ..., -0.46708284,\n",
       "        -0.1649277 , -0.32487997],\n",
       "       [ 2.43334306, -0.64663245,  0.08355601, ..., -0.46708284,\n",
       "        -0.1649277 , -0.32487997],\n",
       "       ...,\n",
       "       [ 0.81094694,  1.61955143, -1.10620282, ..., -0.46708284,\n",
       "        -0.1649277 , -0.32487997],\n",
       "       [ 0.32292332,  1.07607365,  0.80893086, ..., -0.46708284,\n",
       "        -0.1649277 , -0.32487997],\n",
       "       [-1.13732523, -0.50039014,  0.53990795, ..., -0.46708284,\n",
       "        -0.1649277 , -0.32487997]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbf693c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83361291,  2.12312145, -0.41307886, ...,  2.14094782,\n",
       "        -0.1649277 , -0.32487997],\n",
       "       [ 0.8678758 ,  0.62914211, -1.65863345, ..., -0.46708284,\n",
       "        -0.1649277 , -0.32487997],\n",
       "       [ 0.15542089,  0.04777525,  0.10395843, ..., -0.46708284,\n",
       "        -0.1649277 ,  3.07805987],\n",
       "       ...,\n",
       "       [-0.27804892,  0.22364082, -0.23826848, ..., -0.46708284,\n",
       "        -0.1649277 , -0.32487997],\n",
       "       [ 0.38400483, -0.44375139, -1.17226013, ...,  2.14094782,\n",
       "        -0.1649277 , -0.32487997],\n",
       "       [-0.15575346,  0.21847308,  1.36326248, ..., -0.46708284,\n",
       "        -0.1649277 , -0.32487997]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec2daf",
   "metadata": {},
   "source": [
    "## Fit Models\n",
    "Now we will test a range of models. In each we will fit the model in the train data, make predictons for the test data and obtain the accuracy. In later steps we will compare the accuracy of all the models. We will use primarily the library ```sklearn```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f032120",
   "metadata": {},
   "source": [
    "### Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f00a4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1015b55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0dd0f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4015536c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5323590814196242"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb7018d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38344101, 0.61655899],\n",
       "       [0.37556574, 0.62443426],\n",
       "       [0.47435351, 0.52564649],\n",
       "       [0.49680026, 0.50319974],\n",
       "       [0.46441815, 0.53558185],\n",
       "       [0.50314661, 0.49685339],\n",
       "       [0.40750709, 0.59249291],\n",
       "       [0.45330914, 0.54669086],\n",
       "       [0.52968897, 0.47031103],\n",
       "       [0.53191953, 0.46808047],\n",
       "       [0.41948154, 0.58051846],\n",
       "       [0.40308721, 0.59691279],\n",
       "       [0.50307315, 0.49692685],\n",
       "       [0.47368292, 0.52631708],\n",
       "       [0.55040759, 0.44959241],\n",
       "       [0.49815161, 0.50184839],\n",
       "       [0.46738476, 0.53261524],\n",
       "       [0.36143123, 0.63856877],\n",
       "       [0.49936558, 0.50063442],\n",
       "       [0.47304022, 0.52695978],\n",
       "       [0.481564  , 0.518436  ],\n",
       "       [0.45781303, 0.54218697],\n",
       "       [0.46626812, 0.53373188],\n",
       "       [0.37739862, 0.62260138],\n",
       "       [0.54675402, 0.45324598],\n",
       "       [0.52897726, 0.47102274],\n",
       "       [0.48000918, 0.51999082],\n",
       "       [0.57350744, 0.42649256],\n",
       "       [0.34251872, 0.65748128],\n",
       "       [0.46743901, 0.53256099],\n",
       "       [0.41983534, 0.58016466],\n",
       "       [0.52311639, 0.47688361],\n",
       "       [0.47750831, 0.52249169],\n",
       "       [0.44078244, 0.55921756],\n",
       "       [0.51749594, 0.48250406],\n",
       "       [0.58270697, 0.41729303],\n",
       "       [0.45234043, 0.54765957],\n",
       "       [0.48998197, 0.51001803],\n",
       "       [0.4847177 , 0.5152823 ],\n",
       "       [0.47050021, 0.52949979],\n",
       "       [0.62497466, 0.37502534],\n",
       "       [0.55955563, 0.44044437],\n",
       "       [0.28532975, 0.71467025],\n",
       "       [0.48220999, 0.51779001],\n",
       "       [0.53892147, 0.46107853],\n",
       "       [0.47071272, 0.52928728],\n",
       "       [0.56148871, 0.43851129],\n",
       "       [0.39374863, 0.60625137],\n",
       "       [0.50829084, 0.49170916],\n",
       "       [0.47251918, 0.52748082],\n",
       "       [0.40334918, 0.59665082],\n",
       "       [0.54683195, 0.45316805],\n",
       "       [0.46110816, 0.53889184],\n",
       "       [0.54094002, 0.45905998],\n",
       "       [0.41745565, 0.58254435],\n",
       "       [0.52282887, 0.47717113],\n",
       "       [0.50015753, 0.49984247],\n",
       "       [0.46877963, 0.53122037],\n",
       "       [0.42758395, 0.57241605],\n",
       "       [0.44832434, 0.55167566],\n",
       "       [0.48483331, 0.51516669],\n",
       "       [0.50980528, 0.49019472],\n",
       "       [0.46610455, 0.53389545],\n",
       "       [0.41692199, 0.58307801],\n",
       "       [0.41480105, 0.58519895],\n",
       "       [0.40798687, 0.59201313],\n",
       "       [0.27237247, 0.72762753],\n",
       "       [0.48540578, 0.51459422],\n",
       "       [0.56490949, 0.43509051],\n",
       "       [0.48074576, 0.51925424],\n",
       "       [0.42539687, 0.57460313],\n",
       "       [0.51767337, 0.48232663],\n",
       "       [0.46933984, 0.53066016],\n",
       "       [0.45407464, 0.54592536],\n",
       "       [0.48798787, 0.51201213],\n",
       "       [0.50667961, 0.49332039],\n",
       "       [0.42356917, 0.57643083],\n",
       "       [0.4252472 , 0.5747528 ],\n",
       "       [0.46529196, 0.53470804],\n",
       "       [0.43113591, 0.56886409],\n",
       "       [0.56933327, 0.43066673],\n",
       "       [0.43479276, 0.56520724],\n",
       "       [0.40822529, 0.59177471],\n",
       "       [0.51741044, 0.48258956],\n",
       "       [0.46997932, 0.53002068],\n",
       "       [0.48056433, 0.51943567],\n",
       "       [0.39000527, 0.60999473],\n",
       "       [0.56922429, 0.43077571],\n",
       "       [0.46944463, 0.53055537],\n",
       "       [0.48189543, 0.51810457],\n",
       "       [0.50718216, 0.49281784],\n",
       "       [0.47085249, 0.52914751],\n",
       "       [0.44646356, 0.55353644],\n",
       "       [0.48605405, 0.51394595],\n",
       "       [0.43641642, 0.56358358],\n",
       "       [0.40590954, 0.59409046],\n",
       "       [0.54445813, 0.45554187],\n",
       "       [0.61431909, 0.38568091],\n",
       "       [0.48293683, 0.51706317],\n",
       "       [0.47489488, 0.52510512],\n",
       "       [0.39453833, 0.60546167],\n",
       "       [0.53259737, 0.46740263],\n",
       "       [0.46459499, 0.53540501],\n",
       "       [0.46008269, 0.53991731],\n",
       "       [0.47870196, 0.52129804],\n",
       "       [0.58691489, 0.41308511],\n",
       "       [0.4748001 , 0.5251999 ],\n",
       "       [0.55921114, 0.44078886],\n",
       "       [0.48257175, 0.51742825],\n",
       "       [0.42104845, 0.57895155],\n",
       "       [0.60996412, 0.39003588],\n",
       "       [0.54781181, 0.45218819],\n",
       "       [0.5406943 , 0.4593057 ],\n",
       "       [0.48332363, 0.51667637],\n",
       "       [0.50882142, 0.49117858],\n",
       "       [0.50777705, 0.49222295],\n",
       "       [0.24635918, 0.75364082],\n",
       "       [0.54175711, 0.45824289],\n",
       "       [0.43008346, 0.56991654],\n",
       "       [0.45138129, 0.54861871],\n",
       "       [0.48355882, 0.51644118],\n",
       "       [0.43182993, 0.56817007],\n",
       "       [0.54531131, 0.45468869],\n",
       "       [0.45696268, 0.54303732],\n",
       "       [0.45624684, 0.54375316],\n",
       "       [0.46663435, 0.53336565],\n",
       "       [0.47946837, 0.52053163],\n",
       "       [0.47296398, 0.52703602],\n",
       "       [0.5857669 , 0.4142331 ],\n",
       "       [0.51013042, 0.48986958],\n",
       "       [0.49465496, 0.50534504],\n",
       "       [0.4140762 , 0.5859238 ],\n",
       "       [0.39452143, 0.60547857],\n",
       "       [0.48399239, 0.51600761],\n",
       "       [0.46366232, 0.53633768],\n",
       "       [0.48836428, 0.51163572],\n",
       "       [0.48836849, 0.51163151],\n",
       "       [0.58231946, 0.41768054],\n",
       "       [0.49820986, 0.50179014],\n",
       "       [0.50693247, 0.49306753],\n",
       "       [0.41268392, 0.58731608],\n",
       "       [0.55983862, 0.44016138],\n",
       "       [0.49416941, 0.50583059],\n",
       "       [0.50140905, 0.49859095],\n",
       "       [0.49537904, 0.50462096],\n",
       "       [0.6959854 , 0.3040146 ],\n",
       "       [0.44491725, 0.55508275],\n",
       "       [0.46312221, 0.53687779],\n",
       "       [0.40167342, 0.59832658],\n",
       "       [0.43951804, 0.56048196],\n",
       "       [0.44705946, 0.55294054],\n",
       "       [0.52516758, 0.47483242],\n",
       "       [0.43396205, 0.56603795],\n",
       "       [0.60655379, 0.39344621],\n",
       "       [0.52981586, 0.47018414],\n",
       "       [0.4967707 , 0.5032293 ],\n",
       "       [0.54415204, 0.45584796],\n",
       "       [0.49882288, 0.50117712],\n",
       "       [0.47651289, 0.52348711],\n",
       "       [0.53347046, 0.46652954],\n",
       "       [0.53924309, 0.46075691],\n",
       "       [0.30994323, 0.69005677],\n",
       "       [0.48042737, 0.51957263],\n",
       "       [0.53065202, 0.46934798],\n",
       "       [0.53701199, 0.46298801],\n",
       "       [0.58832653, 0.41167347],\n",
       "       [0.33620579, 0.66379421],\n",
       "       [0.60413135, 0.39586865],\n",
       "       [0.4344456 , 0.5655544 ],\n",
       "       [0.56811109, 0.43188891],\n",
       "       [0.40029208, 0.59970792],\n",
       "       [0.52494236, 0.47505764],\n",
       "       [0.46816075, 0.53183925],\n",
       "       [0.46512976, 0.53487024],\n",
       "       [0.45027846, 0.54972154],\n",
       "       [0.48339366, 0.51660634],\n",
       "       [0.48298157, 0.51701843],\n",
       "       [0.4374749 , 0.5625251 ],\n",
       "       [0.51577482, 0.48422518],\n",
       "       [0.48721769, 0.51278231],\n",
       "       [0.59448334, 0.40551666],\n",
       "       [0.53297419, 0.46702581],\n",
       "       [0.55397954, 0.44602046],\n",
       "       [0.48354611, 0.51645389],\n",
       "       [0.51490244, 0.48509756],\n",
       "       [0.52729033, 0.47270967],\n",
       "       [0.4502645 , 0.5497355 ],\n",
       "       [0.42221207, 0.57778793],\n",
       "       [0.45886956, 0.54113044],\n",
       "       [0.51249467, 0.48750533],\n",
       "       [0.49389345, 0.50610655],\n",
       "       [0.37872775, 0.62127225],\n",
       "       [0.54102235, 0.45897765],\n",
       "       [0.46264348, 0.53735652],\n",
       "       [0.51661115, 0.48338885],\n",
       "       [0.64372117, 0.35627883],\n",
       "       [0.46473341, 0.53526659],\n",
       "       [0.47230162, 0.52769838],\n",
       "       [0.44471829, 0.55528171],\n",
       "       [0.61046561, 0.38953439],\n",
       "       [0.53226708, 0.46773292],\n",
       "       [0.47797495, 0.52202505],\n",
       "       [0.53657801, 0.46342199],\n",
       "       [0.34004683, 0.65995317],\n",
       "       [0.46782552, 0.53217448],\n",
       "       [0.49712754, 0.50287246],\n",
       "       [0.40558501, 0.59441499],\n",
       "       [0.40200609, 0.59799391],\n",
       "       [0.63545313, 0.36454687],\n",
       "       [0.47716773, 0.52283227],\n",
       "       [0.76504538, 0.23495462],\n",
       "       [0.54918722, 0.45081278],\n",
       "       [0.42849178, 0.57150822],\n",
       "       [0.44702538, 0.55297462],\n",
       "       [0.51173713, 0.48826287],\n",
       "       [0.5209106 , 0.4790894 ],\n",
       "       [0.56143509, 0.43856491],\n",
       "       [0.38890776, 0.61109224],\n",
       "       [0.40500967, 0.59499033],\n",
       "       [0.51655626, 0.48344374],\n",
       "       [0.50026623, 0.49973377],\n",
       "       [0.34686633, 0.65313367],\n",
       "       [0.4895567 , 0.5104433 ],\n",
       "       [0.49721578, 0.50278422],\n",
       "       [0.56077249, 0.43922751],\n",
       "       [0.42624168, 0.57375832],\n",
       "       [0.49308139, 0.50691861],\n",
       "       [0.55725201, 0.44274799],\n",
       "       [0.51521112, 0.48478888],\n",
       "       [0.45497921, 0.54502079],\n",
       "       [0.49032689, 0.50967311],\n",
       "       [0.44647645, 0.55352355],\n",
       "       [0.44393347, 0.55606653],\n",
       "       [0.50632839, 0.49367161],\n",
       "       [0.38045018, 0.61954982],\n",
       "       [0.56686808, 0.43313192],\n",
       "       [0.48703018, 0.51296982],\n",
       "       [0.47288492, 0.52711508],\n",
       "       [0.43839077, 0.56160923],\n",
       "       [0.52771198, 0.47228802],\n",
       "       [0.66598957, 0.33401043],\n",
       "       [0.62289884, 0.37710116],\n",
       "       [0.39845323, 0.60154677],\n",
       "       [0.56515705, 0.43484295],\n",
       "       [0.48993711, 0.51006289],\n",
       "       [0.43344721, 0.56655279],\n",
       "       [0.56199635, 0.43800365],\n",
       "       [0.40037498, 0.59962502],\n",
       "       [0.46388488, 0.53611512],\n",
       "       [0.47932551, 0.52067449],\n",
       "       [0.441977  , 0.558023  ],\n",
       "       [0.56951094, 0.43048906],\n",
       "       [0.64884333, 0.35115667],\n",
       "       [0.5461365 , 0.4538635 ],\n",
       "       [0.51437803, 0.48562197],\n",
       "       [0.33441238, 0.66558762],\n",
       "       [0.35783641, 0.64216359],\n",
       "       [0.26459076, 0.73540924],\n",
       "       [0.45379175, 0.54620825],\n",
       "       [0.43434609, 0.56565391],\n",
       "       [0.43882237, 0.56117763],\n",
       "       [0.43419526, 0.56580474],\n",
       "       [0.46148422, 0.53851578],\n",
       "       [0.50692316, 0.49307684],\n",
       "       [0.45318105, 0.54681895],\n",
       "       [0.50824331, 0.49175669],\n",
       "       [0.44524437, 0.55475563],\n",
       "       [0.3854284 , 0.6145716 ],\n",
       "       [0.47966501, 0.52033499],\n",
       "       [0.52931214, 0.47068786],\n",
       "       [0.45373814, 0.54626186],\n",
       "       [0.41232611, 0.58767389],\n",
       "       [0.40215661, 0.59784339],\n",
       "       [0.36408515, 0.63591485],\n",
       "       [0.66034835, 0.33965165],\n",
       "       [0.5035038 , 0.4964962 ],\n",
       "       [0.47201768, 0.52798232],\n",
       "       [0.52615219, 0.47384781],\n",
       "       [0.5985966 , 0.4014034 ],\n",
       "       [0.41654795, 0.58345205],\n",
       "       [0.4875393 , 0.5124607 ],\n",
       "       [0.38139938, 0.61860062],\n",
       "       [0.47278227, 0.52721773],\n",
       "       [0.47391093, 0.52608907],\n",
       "       [0.5041774 , 0.4958226 ],\n",
       "       [0.50040225, 0.49959775],\n",
       "       [0.43648371, 0.56351629],\n",
       "       [0.52538531, 0.47461469],\n",
       "       [0.40972705, 0.59027295],\n",
       "       [0.43362844, 0.56637156],\n",
       "       [0.55673858, 0.44326142],\n",
       "       [0.47183522, 0.52816478],\n",
       "       [0.47494542, 0.52505458],\n",
       "       [0.51245155, 0.48754845],\n",
       "       [0.4671823 , 0.5328177 ],\n",
       "       [0.54155647, 0.45844353],\n",
       "       [0.59686792, 0.40313208],\n",
       "       [0.67616428, 0.32383572],\n",
       "       [0.52005263, 0.47994737],\n",
       "       [0.29274395, 0.70725605],\n",
       "       [0.4493316 , 0.5506684 ],\n",
       "       [0.5198298 , 0.4801702 ],\n",
       "       [0.45164646, 0.54835354],\n",
       "       [0.3734887 , 0.6265113 ],\n",
       "       [0.50674018, 0.49325982],\n",
       "       [0.58858255, 0.41141745],\n",
       "       [0.42768346, 0.57231654],\n",
       "       [0.40477151, 0.59522849],\n",
       "       [0.48911017, 0.51088983],\n",
       "       [0.51268111, 0.48731889],\n",
       "       [0.44625645, 0.55374355],\n",
       "       [0.49000635, 0.50999365],\n",
       "       [0.4174517 , 0.5825483 ],\n",
       "       [0.54424406, 0.45575594],\n",
       "       [0.5137229 , 0.4862771 ],\n",
       "       [0.51829603, 0.48170397],\n",
       "       [0.44087461, 0.55912539],\n",
       "       [0.42536561, 0.57463439],\n",
       "       [0.49680332, 0.50319668],\n",
       "       [0.33585284, 0.66414716],\n",
       "       [0.37708826, 0.62291174],\n",
       "       [0.47704977, 0.52295023],\n",
       "       [0.55126856, 0.44873144],\n",
       "       [0.446176  , 0.553824  ],\n",
       "       [0.48072545, 0.51927455],\n",
       "       [0.28939702, 0.71060298],\n",
       "       [0.40786793, 0.59213207],\n",
       "       [0.46443722, 0.53556278],\n",
       "       [0.50280714, 0.49719286],\n",
       "       [0.61903921, 0.38096079],\n",
       "       [0.43802111, 0.56197889],\n",
       "       [0.42366751, 0.57633249],\n",
       "       [0.46765617, 0.53234383],\n",
       "       [0.55669913, 0.44330087],\n",
       "       [0.45022552, 0.54977448],\n",
       "       [0.45545341, 0.54454659],\n",
       "       [0.51653016, 0.48346984],\n",
       "       [0.52159011, 0.47840989],\n",
       "       [0.49153793, 0.50846207],\n",
       "       [0.43755565, 0.56244435],\n",
       "       [0.66225853, 0.33774147],\n",
       "       [0.49765497, 0.50234503],\n",
       "       [0.45047529, 0.54952471],\n",
       "       [0.49352019, 0.50647981],\n",
       "       [0.40324067, 0.59675933],\n",
       "       [0.48824801, 0.51175199],\n",
       "       [0.45158638, 0.54841362],\n",
       "       [0.43034765, 0.56965235],\n",
       "       [0.35735723, 0.64264277],\n",
       "       [0.44436677, 0.55563323],\n",
       "       [0.49483821, 0.50516179],\n",
       "       [0.4268047 , 0.5731953 ],\n",
       "       [0.46267786, 0.53732214],\n",
       "       [0.47686718, 0.52313282],\n",
       "       [0.46398941, 0.53601059],\n",
       "       [0.38136796, 0.61863204],\n",
       "       [0.49078674, 0.50921326],\n",
       "       [0.51852322, 0.48147678],\n",
       "       [0.46055712, 0.53944288],\n",
       "       [0.45831616, 0.54168384],\n",
       "       [0.44914162, 0.55085838],\n",
       "       [0.46645175, 0.53354825],\n",
       "       [0.49393147, 0.50606853],\n",
       "       [0.50043874, 0.49956126],\n",
       "       [0.50072312, 0.49927688],\n",
       "       [0.41981385, 0.58018615],\n",
       "       [0.52380909, 0.47619091],\n",
       "       [0.45002382, 0.54997618],\n",
       "       [0.44162469, 0.55837531],\n",
       "       [0.56184701, 0.43815299],\n",
       "       [0.47349015, 0.52650985],\n",
       "       [0.4318424 , 0.5681576 ],\n",
       "       [0.43530146, 0.56469854],\n",
       "       [0.5767573 , 0.4232427 ],\n",
       "       [0.49938839, 0.50061161],\n",
       "       [0.58126339, 0.41873661],\n",
       "       [0.67092544, 0.32907456],\n",
       "       [0.51924199, 0.48075801],\n",
       "       [0.52176591, 0.47823409],\n",
       "       [0.45819816, 0.54180184],\n",
       "       [0.42851018, 0.57148982],\n",
       "       [0.46697338, 0.53302662],\n",
       "       [0.51905036, 0.48094964],\n",
       "       [0.49712889, 0.50287111],\n",
       "       [0.53111289, 0.46888711],\n",
       "       [0.2611082 , 0.7388918 ],\n",
       "       [0.54628979, 0.45371021],\n",
       "       [0.40484393, 0.59515607],\n",
       "       [0.45575175, 0.54424825],\n",
       "       [0.50683374, 0.49316626],\n",
       "       [0.56544458, 0.43455542],\n",
       "       [0.38973599, 0.61026401],\n",
       "       [0.3852688 , 0.6147312 ],\n",
       "       [0.3039523 , 0.6960477 ],\n",
       "       [0.40316382, 0.59683618],\n",
       "       [0.4717321 , 0.5282679 ],\n",
       "       [0.52650671, 0.47349329],\n",
       "       [0.51667291, 0.48332709],\n",
       "       [0.50341993, 0.49658007],\n",
       "       [0.43363549, 0.56636451],\n",
       "       [0.47786692, 0.52213308],\n",
       "       [0.56065248, 0.43934752],\n",
       "       [0.59122657, 0.40877343],\n",
       "       [0.54746054, 0.45253946],\n",
       "       [0.48417308, 0.51582692],\n",
       "       [0.48232327, 0.51767673],\n",
       "       [0.52795537, 0.47204463],\n",
       "       [0.42190985, 0.57809015],\n",
       "       [0.49033607, 0.50966393],\n",
       "       [0.47881819, 0.52118181],\n",
       "       [0.67747259, 0.32252741],\n",
       "       [0.45671151, 0.54328849],\n",
       "       [0.46450203, 0.53549797],\n",
       "       [0.43470202, 0.56529798],\n",
       "       [0.55928536, 0.44071464],\n",
       "       [0.42070749, 0.57929251],\n",
       "       [0.47151234, 0.52848766],\n",
       "       [0.37432269, 0.62567731],\n",
       "       [0.4666475 , 0.5333525 ],\n",
       "       [0.5683699 , 0.4316301 ],\n",
       "       [0.46689772, 0.53310228],\n",
       "       [0.46104151, 0.53895849],\n",
       "       [0.44385849, 0.55614151],\n",
       "       [0.36773704, 0.63226296],\n",
       "       [0.4268397 , 0.5731603 ],\n",
       "       [0.46379666, 0.53620334],\n",
       "       [0.44266294, 0.55733706],\n",
       "       [0.4887434 , 0.5112566 ],\n",
       "       [0.44596661, 0.55403339],\n",
       "       [0.52253279, 0.47746721],\n",
       "       [0.44475136, 0.55524864],\n",
       "       [0.46922991, 0.53077009],\n",
       "       [0.31312335, 0.68687665],\n",
       "       [0.46045936, 0.53954064],\n",
       "       [0.47730236, 0.52269764],\n",
       "       [0.35151897, 0.64848103],\n",
       "       [0.42513073, 0.57486927],\n",
       "       [0.41270654, 0.58729346],\n",
       "       [0.52212995, 0.47787005],\n",
       "       [0.54959488, 0.45040512],\n",
       "       [0.59757811, 0.40242189],\n",
       "       [0.41282566, 0.58717434],\n",
       "       [0.47668674, 0.52331326],\n",
       "       [0.47004624, 0.52995376],\n",
       "       [0.45399586, 0.54600414],\n",
       "       [0.40074208, 0.59925792],\n",
       "       [0.47880771, 0.52119229],\n",
       "       [0.4830974 , 0.5169026 ],\n",
       "       [0.48159583, 0.51840417],\n",
       "       [0.62811623, 0.37188377],\n",
       "       [0.00489564, 0.99510436],\n",
       "       [0.50089652, 0.49910348],\n",
       "       [0.48010653, 0.51989347],\n",
       "       [0.42491658, 0.57508342],\n",
       "       [0.41233609, 0.58766391],\n",
       "       [0.47840901, 0.52159099],\n",
       "       [0.50399151, 0.49600849],\n",
       "       [0.51615851, 0.48384149],\n",
       "       [0.35888168, 0.64111832],\n",
       "       [0.52728483, 0.47271517],\n",
       "       [0.45770249, 0.54229751],\n",
       "       [0.44660574, 0.55339426],\n",
       "       [0.5313743 , 0.4686257 ],\n",
       "       [0.48664066, 0.51335934],\n",
       "       [0.47527204, 0.52472796],\n",
       "       [0.37236905, 0.62763095],\n",
       "       [0.49579433, 0.50420567],\n",
       "       [0.42585641, 0.57414359],\n",
       "       [0.5322706 , 0.4677294 ],\n",
       "       [0.53370374, 0.46629626],\n",
       "       [0.41903211, 0.58096789],\n",
       "       [0.50516157, 0.49483843],\n",
       "       [0.43817035, 0.56182965],\n",
       "       [0.46662423, 0.53337577],\n",
       "       [0.48531741, 0.51468259],\n",
       "       [0.66813749, 0.33186251],\n",
       "       [0.53586899, 0.46413101],\n",
       "       [0.48732395, 0.51267605],\n",
       "       [0.32582043, 0.67417957]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a8bcc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Accuracy: 0.5323590814196242\n"
     ]
    }
   ],
   "source": [
    "LR_model = LogisticRegression(random_state=1234 , solver='lbfgs')\n",
    "LR_model = LR_model.fit(X_train_scaled, y_train)\n",
    "y_pred_LR = LR_model.predict(X_test_scaled)\n",
    "Accuracy_LR = metrics.accuracy_score(y_test, y_pred_LR)\n",
    "print(\"LR Accuracy:\",Accuracy_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce811b4",
   "metadata": {},
   "source": [
    "### Linear Discrimant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fae80e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Accuracy: 0.5302713987473904\n"
     ]
    }
   ],
   "source": [
    "LDA_model = LinearDiscriminantAnalysis()\n",
    "LDA_model.fit(X_train_scaled,y_train)\n",
    "y_pred_LDA = LDA_model.predict(X_test_scaled)\n",
    "Accuracy_LDA = metrics.accuracy_score(y_test, y_pred_LDA)\n",
    "print(\"LDA Accuracy:\",Accuracy_LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48cf2c",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b62bcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\New folder\\envs\\dev\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy: 0.48643006263048016\n",
      "Hidden layer values:(10, 10, 10)\n",
      "MLP Accuracy: 0.46764091858037576\n",
      "Hidden layer values:(15, 15, 15)\n",
      "MLP Accuracy: 0.5073068893528184\n",
      "Hidden layer values:(20, 20, 20)\n",
      "MLP Accuracy: 0.511482254697286\n",
      "Hidden layer values:(25, 25, 25)\n",
      "MLP Accuracy: 0.5198329853862212\n",
      "Hidden layer values:(30, 30, 30)\n",
      "MLP Accuracy: 0.5073068893528184\n",
      "Hidden layer values:(35, 35, 35)\n",
      "MLP Accuracy: 0.4989561586638831\n",
      "Hidden layer values:(40, 40, 40)\n",
      "MLP Accuracy: 0.48643006263048016\n",
      "Hidden layer values:(45, 45, 45)\n",
      "MLP Accuracy: 0.4989561586638831\n",
      "Hidden layer values:(50, 50, 50)\n",
      "MLP Accuracy: 0.4906054279749478\n",
      "Hidden layer values:(55, 55, 55)\n",
      "MLP Accuracy: 0.5135699373695198\n",
      "Hidden layer values:(60, 60, 60)\n",
      "MLP Accuracy: 0.5386221294363257\n",
      "Hidden layer values:(65, 65, 65)\n",
      "MLP Accuracy: 0.5052192066805845\n",
      "Hidden layer values:(70, 70, 70)\n",
      "MLP Accuracy: 0.5156576200417536\n",
      "Hidden layer values:(75, 75, 75)\n",
      "MLP Accuracy: 0.5260960334029228\n",
      "Hidden layer values:(80, 80, 80)\n",
      "MLP Accuracy: 0.5093945720250522\n",
      "Hidden layer values:(85, 85, 85)\n",
      "MLP Accuracy: 0.49478079331941544\n",
      "Hidden layer values:(90, 90, 90)\n",
      "MLP Accuracy: 0.534446764091858\n",
      "Hidden layer values:(95, 95, 95)\n",
      "MLP Accuracy: 0.5093945720250522\n",
      "Hidden layer values:(100, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,101,5):\n",
    "\n",
    "    MLP_model = MLPClassifier(hidden_layer_sizes=(i,i,i), activation='tanh', solver='adam', max_iter=1500)\n",
    "    MLP_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_MLP = MLP_model.predict(X_test_scaled)\n",
    "    Accuracy_MLP = metrics.accuracy_score(y_test, y_pred_MLP)\n",
    "    print(\"MLP Accuracy:\",Accuracy_MLP)\n",
    "    print(f\"Hidden layer values:{i,i,i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed59d879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy: 0.5031315240083507\n",
      "Hidden layer values:(10, 10, 10)\n",
      "MLP Accuracy: 0.4968684759916493\n",
      "Hidden layer values:(15, 15, 15)\n",
      "MLP Accuracy: 0.48851774530271397\n",
      "Hidden layer values:(20, 20, 20)\n",
      "MLP Accuracy: 0.55741127348643\n",
      "Hidden layer values:(25, 25, 25)\n",
      "MLP Accuracy: 0.5031315240083507\n",
      "Hidden layer values:(30, 30, 30)\n",
      "MLP Accuracy: 0.4968684759916493\n",
      "Hidden layer values:(35, 35, 35)\n",
      "MLP Accuracy: 0.5031315240083507\n",
      "Hidden layer values:(40, 40, 40)\n",
      "MLP Accuracy: 0.5302713987473904\n",
      "Hidden layer values:(45, 45, 45)\n",
      "MLP Accuracy: 0.5302713987473904\n",
      "Hidden layer values:(50, 50, 50)\n",
      "MLP Accuracy: 0.534446764091858\n",
      "Hidden layer values:(55, 55, 55)\n",
      "MLP Accuracy: 0.5010438413361169\n",
      "Hidden layer values:(60, 60, 60)\n",
      "MLP Accuracy: 0.49269311064718163\n",
      "Hidden layer values:(65, 65, 65)\n",
      "MLP Accuracy: 0.5177453027139874\n",
      "Hidden layer values:(70, 70, 70)\n",
      "MLP Accuracy: 0.49478079331941544\n",
      "Hidden layer values:(75, 75, 75)\n",
      "MLP Accuracy: 0.511482254697286\n",
      "Hidden layer values:(80, 80, 80)\n",
      "MLP Accuracy: 0.5177453027139874\n",
      "Hidden layer values:(85, 85, 85)\n",
      "MLP Accuracy: 0.5198329853862212\n",
      "Hidden layer values:(90, 90, 90)\n",
      "MLP Accuracy: 0.5302713987473904\n",
      "Hidden layer values:(95, 95, 95)\n",
      "MLP Accuracy: 0.534446764091858\n",
      "Hidden layer values:(100, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,101,5):\n",
    "\n",
    "    MLP_model = MLPClassifier(hidden_layer_sizes=(i,i,i), activation='relu', solver='adam', max_iter=1500)\n",
    "    MLP_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_MLP = MLP_model.predict(X_test_scaled)\n",
    "    Accuracy_MLP = metrics.accuracy_score(y_test, y_pred_MLP)\n",
    "    print(\"MLP Accuracy:\",Accuracy_MLP)\n",
    "    print(f\"Hidden layer values:{i,i,i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6699587e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy: 0.5198329853862212\n"
     ]
    }
   ],
   "source": [
    "MLP_model = MLPClassifier(hidden_layer_sizes=(25,25,25), activation='relu', solver='adam', max_iter=1500)\n",
    "MLP_model.fit(X_train_scaled, y_train)\n",
    "y_pred_MLP = MLP_model.predict(X_test_scaled)\n",
    "Accuracy_MLP = metrics.accuracy_score(y_test, y_pred_MLP)\n",
    "print(\"MLP Accuracy:\",Accuracy_MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9518393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869ef73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6732f7d9",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = [Accuracy_LR, Accuracy_LDA, Accuracy_MLP]\n",
    "\n",
    "model_list = ['Logistic Regression', 'Linear Discriminat','Neural Network']\n",
    "\n",
    "df_accuracy = pd.DataFrame({'Model': model_list, 'Accuracy': accuracy_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = list(df_accuracy.sort_values('Accuracy', ascending=False).Model)\n",
    "df_accuracy = df_accuracy.sort_values('Accuracy', ascending=False).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "# make barplot and sort bars\n",
    "x = sns.barplot(x='Model', y=\"Accuracy\", data=df_accuracy, order = order, palette=\"rocket\")\n",
    "plt.xlabel(\"Model\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", fontsize=20)\n",
    "plt.title(\"Accuracy by Model\", fontsize=20)\n",
    "plt.grid(linestyle='-', linewidth='0.5', color='grey')\n",
    "plt.xticks(rotation=70, fontsize=12)\n",
    "plt.ylim(0,1)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "for i in range(len(model_list)):\n",
    "    plt.text(x = i, y = df_accuracy.loc[i, 'Accuracy'] + 0.05, s = str(round((df_accuracy.loc[i, 'Accuracy'])*100, 2))+'%', \n",
    "             fontsize = 14, color='black',horizontalalignment='center')\n",
    "\n",
    "#y_value=['{:,.2f}'.format(x) + '%' for x in ax.get_yticks()]\n",
    "#ax.set_yticklabels(y_value)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c550f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
