{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896fa4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependancies\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from yahoo_fin.stock_info import *\n",
    "import yfinance as yf\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "from pprint import pprint\n",
    "import finnhub\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# change to your api keys \n",
    "\n",
    "#alphavantage_api_key = 'AJP4EKR4LE52NNK4'\n",
    "alphavantage_api_key = 'CORYDT1RMNVL3DVT'\n",
    "polygon_api_key = 'J9UUfkyTylf3D_s76b5pcMCnLSaKTNpD'\n",
    "finnhub_client = finnhub.Client(api_key=\"cld35g9r01qn959jif1gcld35g9r01qn959jif20\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3f812",
   "metadata": {},
   "source": [
    "### Screening US Mega & Large Cap Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2655c450",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nasdaq_screener_1700335987448.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnasdaq_screener_1700335987448.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a Pandas DataFrame\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m stock_list_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m      7\u001b[0m stock_list_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nasdaq_screener_1700335987448.csv'"
     ]
    }
   ],
   "source": [
    "file_path = 'nasdaq_screener_1700335987448.csv'\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "stock_list_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "stock_list_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of tickers\n",
    "\n",
    "ticker_list = stock_list_df['Symbol'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9bc47",
   "metadata": {},
   "source": [
    "### Generating functions to retrieve stock information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e941fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate random date, and ensure that the date is a weekday, maybe start in 2014 or 2015\n",
    "\n",
    "def generate_random_date(start_date, end_date):\n",
    "    check_weekday = False\n",
    "    while not check_weekday:\n",
    "        # Convert the date strings to datetime objects\n",
    "        start_date_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end_date_dt = datetime.datetime.now()  # Using today's date as the end date\n",
    "\n",
    "        # Calculate the range of days between the start and end date\n",
    "        delta_days = (end_date_dt - start_date_dt).days\n",
    "\n",
    "        # Generate a random number of days within the range\n",
    "        random_days = random.randint(0, delta_days)\n",
    "\n",
    "        # Calculate the random date\n",
    "        random_date = start_date_dt + timedelta(days=random_days)\n",
    "\n",
    "        # Check if the generated date is a weekday\n",
    "        if random_date.weekday() < 5:\n",
    "            check_weekday = True\n",
    "            return random_date\n",
    "        else:\n",
    "            check_weekday = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3610f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to choose a random stock from the list\n",
    "\n",
    "def choose_random_stock(ticker_list):    \n",
    "    \n",
    "    # Randomly choose a stock from the list\n",
    "    random_stock = random.choice(ticker_list)\n",
    "    \n",
    "    return random_stock.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d648cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check if data exists for stock on the date \n",
    "\n",
    "def check_stock_date(ticker, check_date):\n",
    "    check_financials = False\n",
    "    # Check if the input date is a Friday\n",
    "    if check_date.weekday() == 4:  # Friday\n",
    "        # Set end date to the next Monday\n",
    "        adj_end_date = check_date + timedelta(days=3)\n",
    "    else:\n",
    "        # Set end date to the next day\n",
    "        adj_end_date = check_date + timedelta(days=1)\n",
    "    \n",
    "    # using yahoo finance function to get stock data dataframe\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        get_data(ticker, end_date=adj_end_date)\n",
    "        get_stock_financials(ticker, check_date)\n",
    "        \n",
    "\n",
    "        check_financials = True\n",
    "    except:\n",
    "        check_financials = False\n",
    "    \n",
    "    if not check_financials:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to return stock industry\n",
    "\n",
    "def return_stock_industry(symbol):\n",
    "    symbol = symbol.upper()\n",
    "    # Check if the symbol is in the DataFrame\n",
    "    if symbol in stock_list_df['Symbol'].values:\n",
    "        # Retrieve the sector for the given symbol\n",
    "        sector = stock_list_df.loc[stock_list_df['Symbol'] == symbol, 'Sector'].iloc[0]\n",
    "        return sector\n",
    "    else:\n",
    "        return f\"Symbol '{symbol}' not found in the stock list.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9608205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function to get the adjusted closing price of a stock on a particular day\n",
    "\n",
    "def get_stock_price(ticker, stock_date):\n",
    "    # Check if the input date is a Friday\n",
    "    if stock_date.weekday() == 4:  # Friday\n",
    "        # Set end date to the next Monday\n",
    "        price_adj_end_date = stock_date + timedelta(days=3)\n",
    "    else:\n",
    "        # Set end date to the next day\n",
    "        price_adj_end_date = stock_date + timedelta(days=1)\n",
    "    \n",
    "    # using yahoo finance function to get stock data dataframe\n",
    "    stock_data = get_data(ticker, end_date=price_adj_end_date)\n",
    "    # extracting the price \n",
    "    adj_close_price = stock_data['adjclose'].values[-1]\n",
    "    \n",
    "    return adj_close_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get stock trading volume\n",
    "\n",
    "def get_stock_volume(ticker, vol_date):\n",
    "    # Check if the input date is a Friday\n",
    "    if vol_date.weekday() == 4:  # Friday\n",
    "        # Set end date to the next Monday\n",
    "        vol_adj_end_date = vol_date + timedelta(days=3)\n",
    "    else:\n",
    "        # Set end date to the next day\n",
    "        vol_adj_end_date = vol_date + timedelta(days=1)\n",
    "    \n",
    "    # using yahoo finance function to get stock data dataframe\n",
    "    stock_data = get_data(ticker, end_date=vol_adj_end_date)\n",
    "    \n",
    "    #extracting the volume\n",
    "    trading_vol = stock_data['volume'].values[-1]\n",
    "    \n",
    "    return trading_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e42820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get 4 previous weekdays from given_date\n",
    "\n",
    "def get_previous_weekdays(weekday_date):\n",
    "    previous_weekdays = []\n",
    "    previous_date = weekday_date - timedelta(days=1)\n",
    "    while len(previous_weekdays) < 4:\n",
    "        if previous_date.weekday() < 5:\n",
    "            previous_weekdays.append(previous_date)\n",
    "        previous_date -= timedelta(days=1)\n",
    "\n",
    "    return previous_weekdays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f02233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get company's financial ratios by scraping stockanalysis.com\n",
    "\n",
    "def get_stock_financials(ticker, fin_date):\n",
    "    \n",
    "    financial_ratios = {}\n",
    "\n",
    "    # setting brwoser\n",
    "    browser = Browser('chrome')\n",
    "    \n",
    "    # URL of the website\n",
    "    url = f\"https://stockanalysis.com/stocks/{ticker}/financials/ratios/?p=quarterly\"\n",
    "\n",
    "    # Use Selenium to open the webpage and let JavaScript execute\n",
    "    driver = webdriver.Chrome()  # Make sure you have the ChromeDriver executable in your PATH\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow some time for JavaScript to execute and load the content\n",
    "    time.sleep(5)  # Adjust the sleep time based on your needs\n",
    "\n",
    "    # Get the page source after JavaScript execution\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Close the Selenium WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", class_=\"w-full\")\n",
    "\n",
    "    # Check if the table is found\n",
    "    try:\n",
    "        # Use pandas to read the HTML table into a DataFrame\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        # Extract date columns excluding the first column\n",
    "        date_columns = df.columns[2:-1]\n",
    "        \n",
    "        # Convert date columns to datetime objects\n",
    "        date_columns = pd.to_datetime(date_columns)\n",
    "        \n",
    "        # Find the closest date column to the given date that is earlier\n",
    "        earlier_dates = [date for date in date_columns if date < fin_date]\n",
    "        closest_date_column = max(earlier_dates)\n",
    "\n",
    "        # Access the closest column\n",
    "        closest_column = closest_date_column.strftime('%Y-%m-%d')\n",
    "        \n",
    "    except:\n",
    "        print(\"Table not found on the page.\")\n",
    "    \n",
    "    \n",
    "\n",
    "    #extracting financial ratios\n",
    "    try: \n",
    "        financial_ratios['pe_ratio']=float(df.loc[df['Quarter Ended'] == 'PE Ratio', closest_column].values[0])\n",
    "        financial_ratios['debt_to_equity']=float(df.loc[df['Quarter Ended'] == 'Debt / Equity Ratio', closest_column].values[0])\n",
    "        financial_ratios['quick_ratio']=float(df.loc[df['Quarter Ended'] == 'Quick Ratio', closest_column].values[0])\n",
    "        total_shareholder_return_str = df.loc[df['Quarter Ended'] == 'Total Shareholder Return', closest_column].values[0]\n",
    "        financial_ratios['total_shareholder_return'] = float(total_shareholder_return_str.strip('%'))\n",
    "    \n",
    "    except:\n",
    "        pass \n",
    "    \n",
    "    # URL of the website\n",
    "    url = f\"https://stockanalysis.com/stocks/{ticker}/financials/?p=quarterly\"\n",
    "\n",
    "    # Use Selenium to open the webpage and let JavaScript execute\n",
    "    driver = webdriver.Chrome()  # Make sure you have the ChromeDriver executable in your PATH\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow some time for JavaScript to execute and load the content\n",
    "    time.sleep(5)  # Adjust the sleep time based on your needs\n",
    "\n",
    "    # Get the page source after JavaScript execution\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Close the Selenium WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    # Find the table by class name\n",
    "    table = soup.find(\"table\", class_=\"w-full\")\n",
    "\n",
    "    # Check if the table is found\n",
    "    try:\n",
    "    # Use pandas to read the HTML table into a DataFrame\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        \n",
    "    except:\n",
    "        print(\"Table not found on the page.\")\n",
    "    \n",
    "    try:\n",
    "        profit_margin_str= df.loc[df['Quarter Ended'] == 'Profit Margin', closest_column].values[0]\n",
    "        financial_ratios['profit_margin'] = float(profit_margin_str.strip('%'))\n",
    "        cash_margin_str= df.loc[df['Quarter Ended'] == 'Free Cash Flow Margin', closest_column].values[0]\n",
    "        financial_ratios['free_cash_margin'] = float(cash_margin_str.strip('%'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return financial_ratios\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb220ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate stock's volatility\n",
    "\n",
    "def calculate_volatility(ticker, sd_date):\n",
    "    df=get_data(ticker, end_date=sd_date)\n",
    "    # Calculate daily returns\n",
    "    df['daily_return'] = df['adjclose'].pct_change()\n",
    "\n",
    "    # Calculate volatility as the standard deviation of daily returns\n",
    "    volatility = df['daily_return'].std()\n",
    "\n",
    "    return volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbb828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------NEED TO FIND A NEW SOURCE---------------------------------\n",
    "\n",
    "def get_news_sentiment(ticker, news_date):\n",
    "    \n",
    "    # Extract year, month, and day\n",
    "    news_end_date=news_date\n",
    "    end_year = news_end_date.year\n",
    "    end_month = str(news_end_date.month).zfill(2)\n",
    "    end_day = str(news_end_date.day).zfill(2)\n",
    "    \n",
    "    news_start_date = news_end_date - timedelta(weeks=4)\n",
    "    start_year = news_start_date.year\n",
    "    start_month = str(news_start_date.month).zfill(2)\n",
    "    start_day = str(news_start_date.day).zfill(2)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7903b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting daily federal interest rate information from alphavantage\n",
    "\n",
    "# using alphavantage API\n",
    "url = f'https://www.alphavantage.co/query?function=FEDERAL_FUNDS_RATE&interval=daily&apikey={alphavantage_api_key}'\n",
    "r = requests.get(url)\n",
    "data = r.json()\n",
    "\n",
    "daily_interest_rates = data\n",
    "\n",
    "# converting json to dataframe\n",
    "daily_interest_rate_df = pd.DataFrame(daily_interest_rates['data'])\n",
    "\n",
    "# Export DataFrame to CSV so that we can refer to it without making an api call\n",
    "daily_interest_rate_df.to_csv('daily_interest_rates.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85664b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting monthly CPI information from alphavantage\n",
    "\n",
    "# using alphavantage api\n",
    "url = f'https://www.alphavantage.co/query?function=CPI&interval=monthly&apikey=demo'\n",
    "r = requests.get(url)\n",
    "data = r.json()\n",
    "\n",
    "monthly_cpi = data\n",
    "\n",
    "# converting json to dataframe\n",
    "monthly_cpi_df = pd.DataFrame(monthly_cpi['data'])\n",
    "monthly_cpi_df['date'] = pd.to_datetime(monthly_cpi_df['date'])\n",
    "\n",
    "\n",
    "# Export DataFrame to CSV so that we can refer to it without making an api call\n",
    "monthly_cpi_df.to_csv('monthly_cpi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51961146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting monthly unemployment rate information from alphavantage\n",
    "\n",
    "# using alphavantage api\n",
    "url = f'https://www.alphavantage.co/query?function=UNEMPLOYMENT&apikey=demo'\n",
    "r = requests.get(url)\n",
    "data = r.json()\n",
    "\n",
    "monthly_unemployment_rate = data\n",
    "\n",
    "# converting json to dataframe\n",
    "monthly_unemployment_rate_df = pd.DataFrame(monthly_unemployment_rate['data'])\n",
    "monthly_unemployment_rate_df['date'] = pd.to_datetime(monthly_unemployment_rate_df['date'])\n",
    "\n",
    "# Export DataFrame to CSV so that we can refer to it without making an api call\n",
    "monthly_unemployment_rate_df.to_csv('monthly_unemployment_rate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef26aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to extract economic indicators for the given date\n",
    "\n",
    "def get_economic_indicators(econ_date):\n",
    "    \n",
    "    economic_indicators_dict={}\n",
    "    \n",
    "    # convert datetime object to matching string \n",
    "    formatted_date = econ_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # retrieve CPI value\n",
    "    # Locate the value for the given date, matching only month and year\n",
    "    economic_indicators_dict['cpi'] = float(monthly_cpi_df.loc[monthly_cpi_df['date'].dt.to_period('M') == pd.to_datetime(econ_date).to_period('M'), 'value'].iloc[0])    \n",
    "    \n",
    "    # retrieve federal interest rate value\n",
    "    economic_indicators_dict['interest_rate'] = float(daily_interest_rate_df.loc[daily_interest_rate_df['date'] == formatted_date, 'value'].iloc[0])\n",
    "\n",
    "    # retrieve unemployement rate\n",
    "    # Locate the value for the given date, matching only month and year\n",
    "    economic_indicators_dict['unemployment_rate'] = float(monthly_unemployment_rate_df.loc[monthly_unemployment_rate_df['date'].dt.to_period('M') == pd.to_datetime(econ_date).to_period('M'), 'value'].iloc[0])\n",
    "    \n",
    "    return economic_indicators_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve technical indicators\n",
    "\n",
    "def get_technical_indicators(ticker, tech_date):\n",
    "    \n",
    "    technical_indicators_dict={}\n",
    "    \n",
    "    # changing date into UNIX format\n",
    "    end_timestamp = int(tech_date.timestamp())\n",
    "    \n",
    "    # choosing a timestamp of one week earlier\n",
    "    start_tech_date = tech_date - timedelta(weeks=1)\n",
    "    \n",
    "    # changing the date to UNIX format\n",
    "    start_timestamp = int(start_tech_date.timestamp())\n",
    "    \n",
    "    # retrieving SMA \n",
    "    technical_indicators_dict['sma']= finnhub_client.technical_indicator(symbol=ticker, resolution='D', _from=start_timestamp, to=end_timestamp, indicator='sma', indicator_fields={\"timeperiod\": 5})['sma'][-1]\n",
    "    \n",
    "    #retrieving RSI\n",
    "    technical_indicators_dict['rsi']= finnhub_client.technical_indicator(symbol=ticker, resolution='D', _from=start_timestamp, to=end_timestamp, indicator='rsi', indicator_fields={\"timeperiod\": 5})['rsi'][-1]\n",
    "\n",
    "    # retrieving EMA \n",
    "    technical_indicators_dict['ema']= finnhub_client.technical_indicator(symbol=ticker, resolution='D', _from=start_timestamp, to=end_timestamp, indicator='ema', indicator_fields={\"timeperiod\": 5})['ema'][-1]\n",
    "    \n",
    "    return technical_indicators_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a499ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create label value for data\n",
    "\n",
    "def create_label(ticker, y_date):\n",
    "    print(y_date)\n",
    "    initial_price = get_stock_price(ticker, y_date)\n",
    "    print(initial_price)\n",
    "    \n",
    "    # find the next weekday\n",
    "    \n",
    "    # check if date landed on friday and calculate the next weekday date\n",
    "    if y_date.weekday() == 4:\n",
    "        next_weekday_date = y_date + timedelta(days=3)\n",
    "    else:\n",
    "        next_weekday_date = y_date + timedelta(days=1)\n",
    "\n",
    "    print(next_weekday_date)\n",
    "    \n",
    "    next_price = get_stock_price(ticker, next_weekday_date)\n",
    "    \n",
    "    print(next_price)\n",
    "    \n",
    "    if next_price >= initial_price:\n",
    "        return 1 # 1 is class increase, in order to simplify the classification we put stayed the same in same category as increase\n",
    "    else:\n",
    "        return 0 # 0 is class decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de6993",
   "metadata": {},
   "source": [
    "### Running a loop to generate multiple rows of data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "page=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e12b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store dictionaries\n",
    "list_dict = []\n",
    "page += 1\n",
    "# runs loop until list_dic [change no. 4 as desired]\n",
    "while len(list_dict) < 500:\n",
    "\n",
    "    # data_dict\n",
    "    data_dict = {}\n",
    "    \n",
    "    # checking to see if generated ticker-date combo has available data\n",
    "    checker = False\n",
    "    \n",
    "    # runs this loop until checker is true\n",
    "    while not checker:\n",
    "\n",
    "        ticker = choose_random_stock(ticker_list)\n",
    "        input_date = generate_random_date('2015-01-01', '2023-10-31')\n",
    "\n",
    "        checker = check_stock_date(ticker, input_date)\n",
    "\n",
    "    # using function to return stock industry\n",
    "    data_dict['industry']=return_stock_industry(ticker)\n",
    "\n",
    "    # retrieve weekly returns\n",
    "    date = input_date\n",
    "    for i in range(1, 5):\n",
    "        data_dict[f'wr{i}']=(get_stock_price(ticker, date) - get_stock_price(ticker, date - timedelta(weeks = 1)))/get_stock_price(ticker, date - timedelta(weeks = 1))\n",
    "        date = date - timedelta(weeks=1)\n",
    "\n",
    "    # retrieve last 5 working day volumes\n",
    "    data_dict['vol1'] = get_stock_volume(ticker, date)\n",
    "    for i, weekday in enumerate(get_previous_weekdays(input_date)):\n",
    "        data_dict[f'vol{i+1}'] = get_stock_volume(ticker, weekday)\n",
    "\n",
    "    # retrieve stock financials    \n",
    "    for key, value in get_stock_financials(ticker, input_date).items():\n",
    "        data_dict[key] = value\n",
    "    \n",
    "    # retrieve stock volatility\n",
    "    data_dict['volatility'] = calculate_volatility(ticker, input_date)\n",
    "\n",
    "    # retrieve economic indicators\n",
    "    for key, value in get_economic_indicators(input_date).items():\n",
    "        data_dict[key] = value\n",
    "\n",
    "    # retrieve stocktechnical indicators    \n",
    "    for key, value in get_technical_indicators(ticker, input_date).items():\n",
    "        data_dict[key] = value\n",
    "\n",
    "    # generate a label for row    \n",
    "    data_dict['label'] = create_label(ticker, input_date)\n",
    "        \n",
    "    # append dictionary to list\n",
    "    list_dict.append(data_dict)\n",
    "    print(len(list_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5658841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0253c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ticker, input_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac18d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(input_date).to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6874f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_cpi_df.loc[monthly_cpi_df['date'].dt.to_period('M') == pd.to_datetime(input_date).to_period('M'), 'value'].iloc[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5374ca",
   "metadata": {},
   "source": [
    "### Export data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3142db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to export data as csv\n",
    "data_df = pd.DataFrame(list_dict)\n",
    "data_df.to_csv(f'data{page}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b61473",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ticker, input_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c092c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
